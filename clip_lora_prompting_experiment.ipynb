{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# CLIP LoRA Prompting Experiment Notebook\n\nThis notebook mirrors `experiments/clip_lora_prompting_experiment.py` while adding short notes so every logical block is easier to follow."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "> **Tip:** The notebook resets `sys.argv` before running the main experiment so that the argument parser sees the same defaults you would get when executing the Python script from the command line. Adjust the arguments manually if you want to experiment with different settings."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import argparse\nimport csv\nimport json\nimport os\nimport sys\nfrom collections import defaultdict\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Dict, List, Optional\n\nimport torch\nfrom peft import LoraConfig, LoraModel\n\nfrom experiments.utils import compute_init_ctx, export_family_embeddings\n\nos.environ.setdefault(\"NUMBA_DISABLE_CACHING\", \"1\")\n\ntry:\n    NOTEBOOK_PATH = Path(__file__).resolve()\n    PROJECT_ROOT = NOTEBOOK_PATH.parent.parent\nexcept NameError:\n    PROJECT_ROOT = Path.cwd().resolve()\n\nif str(PROJECT_ROOT) not in sys.path:\n    sys.path.insert(0, str(PROJECT_ROOT))\n\nfrom ecogrow.benchmark.ecogrow_benchmark import EcogrowBenchmark\nfrom ecogrow.models.open_clip_wrapper import init_open_clip, FamilyAdaptedClipDetector\nfrom ecogrow.preprocessing.image_segmentator import (\n    black_bg_composite,\n    crop_to_alpha_bbox,\n    segment_plant_rgba,\n)\nfrom ecogrow.training.prompt_learners import ClipPromptLearner\nfrom ecogrow.training.trainers import ClipFineTuneEngine\nfrom ecogrow.data.plant_data import PlantData, make_segment_fn, DISEASE_MAPPING\nfrom ecogrow.models.checkpoint_cache import ensure_mobileclip_checkpoint\n\n_last_run_context = None\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Configuration Handling\n\nWe keep the same dataclass and CLI parsing logic from the script so the training run behaves identically regardless of whether it is launched from a terminal or this notebook."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "@dataclass(frozen=True)\nclass Config:\n    dataset_path: Path\n    embeddings_dir: Optional[Path]\n    prompts_config: Path\n    run_id: str\n    exp_dir: Path\n    epochs: int\n    batch_size: int\n    perc_eval: float\n    lr: float\n    classifier_dropout: float\n\n\ndef _parse_args() -> Config:\n    parser = argparse.ArgumentParser(description=\"EcoGrow CLIP fine-tuning experiment\")\n    parser.add_argument(\n        \"--dataset-path\",\n        default=\"datasets\",\n        help=\"Percorso della directory del dataset (es. data/Indoor-Plant-disease-dataset-1)\",\n    )\n    parser.add_argument(\n        \"--embeddings-dir\",\n        default=\"artifacts/embeddings\",\n        help=\"Directory dove salvare gli embedding testuali esportati (per l'inferenza).\",\n    )\n    parser.add_argument(\n        \"--prompts-config\",\n        default=\"experiments/prompts.json\",\n        help=\"File JSON con la configurazione delle famiglie e relative classi\",\n    )\n    parser.add_argument(\n        \"--run-id\",\n        default=None,\n        help=\"Identificativo della run; se non specificato viene derivato dal file di prompt.\",\n    )\n    parser.add_argument(\n        \"--exp-dir\",\n        default=\"experiments\",\n        help=\"Directory principale dove salvare i risultati.\",\n    )\n    parser.add_argument(\n        \"--epochs\",\n        type=int,\n        default=10,\n        help=\"Numero di epoche di fine-tuning per ciascuna famiglia.\",\n    )\n    parser.add_argument(\n        \"--batch-size\",\n        type=int,\n        default=8,\n        help=\"Batch size utilizzata durante il fine-tuning.\",\n    )\n    parser.add_argument(\n        \"--perc-eval\",\n        type=float,\n        default=0.2,\n        help=\"Frazione del training da usare come validation (0 disabilita lo split).\",\n    )\n    parser.add_argument(\n        \"--lr\",\n        type=float,\n        default=5e-4,\n        help=\"Learning rate per l'ottimizzatore AdamW.\",\n    )\n    parser.add_argument(\n        \"--classifier-dropout\",\n        type=float,\n        default=0.1,\n        help=\"Dropout applicato prima del classificatore lineare.\",\n    )\n    args = parser.parse_args()\n\n    dataset_path = Path(args.dataset_path).expanduser().resolve()\n    if not dataset_path.is_dir():\n        raise FileNotFoundError(f\"Dataset path '{dataset_path}' non esiste o non \u00e8 una directory.\")\n\n    embeddings_dir = None\n    if args.embeddings_dir:\n        embeddings_dir = Path(args.embeddings_dir).expanduser().resolve()\n        embeddings_dir.mkdir(parents=True, exist_ok=True)\n\n    prompts_path = Path(args.prompts_config).expanduser().resolve()\n    if not prompts_path.is_file():\n        raise FileNotFoundError(f\"Prompts config '{prompts_path}' non esiste.\")\n\n    exp_dir = Path(args.exp_dir).expanduser().resolve()\n    exp_dir.mkdir(parents=True, exist_ok=True)\n\n    run_id = args.run_id or f\"clip_lora_finetuning_{prompts_path.stem}\"\n\n    return Config(\n        dataset_path=dataset_path,\n        embeddings_dir=embeddings_dir,\n        prompts_config=prompts_path,\n        run_id=run_id,\n        exp_dir=exp_dir,\n        epochs=args.epochs,\n        batch_size=args.batch_size,\n        perc_eval=max(0.0, float(args.perc_eval)),\n        lr=args.lr,\n        classifier_dropout=max(0.0, float(args.classifier_dropout)),\n    )"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Prompt Helpers\n\nThe following utilities manipulate the prompt configuration exactly as in the script so that downstream training uses the same class ordering."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def _canonicalize_label(label: str) -> str:\n    normalized = label.replace(\"-\", \"_\").replace(\" \", \"_\").lower()\n    for alias, canonical in DISEASE_MAPPING.items():\n        if alias in normalized:\n            return canonical\n    return normalized\n\n\ndef _collect_class_prompts(prompt_config: Dict[str, object]) -> Dict[str, List[str]]:\n    \"\"\"Extract prompt texts per disease class from the JSON config.\"\"\"\n\n    per_class: Dict[str, List[str]] = defaultdict(list)\n\n    def _append(label: str, value) -> None:\n        if not label:\n            return\n        canonical = _canonicalize_label(label)\n\n        if isinstance(value, str):\n            text = value.strip()\n            if text:\n                per_class[canonical].append(text)\n        elif isinstance(value, (list, tuple, set)):\n            for item in value:\n                _append(label, item)\n        elif isinstance(value, dict):\n            for nested in value.values():\n                _append(label, nested)\n\n    for family_payload in prompt_config.values():\n        if isinstance(family_payload, dict):\n            for disease_label, entries in family_payload.items():\n                _append(disease_label, entries)\n\n    return per_class\n\n\ndef _default_prompt(label: str) -> str:\n    pretty = label.replace(\"_\", \" \")\n    return f\"a close-up photo of a plant showing {pretty}\""
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Run the Experiment\n\nThe function below is a literal translation of the script's `main` routine, with only notebook-friendly tweaks. Comments highlight why the steps happen."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def run_clip_lora_prompting_experiment() -> Dict[str, Dict[str, object]]:\n    global _last_run_context\n    config = _parse_args()\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model_name = \"MobileCLIP-S2\"\n    pretrained_tag = ensure_mobileclip_checkpoint(model_name=model_name)\n    clip_model, preprocess, tokenizer, text_encoder = init_open_clip(\n        model_name=model_name,\n        pretrained_tag=pretrained_tag,\n        device=device,\n    )\n\n    candidate_targets = [\n        \"token_mixer.qkv\",\n        \"token_mixer.proj\",\n        \"head.fc\"\n    ]\n\n    submodule_names = [name for name, _ in clip_model.visual.named_modules()]\n    filtered_targets = [t for t in candidate_targets if any(t in n for n in submodule_names)]\n    if not filtered_targets:\n        fallback = [\"attn.qkv\", \"attn.proj\", \"mlp.fc1\", \"mlp.fc2\", \"qkv\", \"proj\"]\n        filtered_targets = [t for t in fallback if any(t in n for n in submodule_names)]\n\n    print(f\"[LoRA] target_modules candidates matched: {filtered_targets if filtered_targets else 'NONE'}\")\n\n    base_visual = clip_model.visual\n    for p in base_visual.parameters():\n        p.requires_grad_(False)\n\n    lora_config = LoraConfig(\n        r=8,\n        lora_alpha=16,\n        lora_dropout=0.05,\n        target_modules=filtered_targets if filtered_targets else candidate_targets,\n        bias=\"none\",\n        task_type=\"FEATURE_EXTRACTION\",\n    )\n\n    clip_model.visual = LoraModel(base_visual, lora_config, adapter_name=\"default\")\n\n    num_adapters = sum(1 for m in clip_model.visual.modules() if hasattr(m, \"lora_A\") and hasattr(m, \"lora_B\"))\n    num_lora_params = sum(p.numel() for n, p in clip_model.visual.named_parameters() if p.requires_grad and \"lora_\" in n)\n    total_trainable_visual = sum(p.numel() for p in clip_model.visual.parameters() if p.requires_grad)\n    print(f\"[LoRA] adapters inserted: {num_adapters}\")\n    print(f\"[LoRA] trainable LoRA params: {num_lora_params}\")\n    print(f\"[LoRA] total trainable params in visual: {total_trainable_visual}\")\n\n    benchmark = EcogrowBenchmark(\n        run_id=config.run_id,\n        exp_dir=str(config.exp_dir),\n        data_root=str(config.dataset_path),\n    )\n\n    segment_fn = make_segment_fn(\n        segment_plant_rgba,\n        crop_to_alpha_bbox,\n        black_bg_composite,\n        pad=12,\n    )\n    with open(config.prompts_config, \"r\", encoding=\"utf-8\") as f:\n        prompt_config = json.load(f)\n\n    families = tuple(dict.fromkeys(prompt_config.keys()))\n    if not families:\n        raise ValueError(\"Prompts config must define at least one family.\")\n\n    preview_dataset = PlantData(\n        dataset_root=config.dataset_path,\n        families=families,\n        split=\"train\",\n        segment_fn=segment_fn,\n        transform=preprocess,\n    )\n    classnames = preview_dataset.classes\n\n    prompt_texts_map = _collect_class_prompts(prompt_config)\n    class_prompt_texts: List[str] = []\n    for cls in classnames:\n        prompts_for_class = prompt_texts_map.get(cls)\n        if prompts_for_class:\n            class_prompt_texts.append(prompts_for_class[0])\n        else:\n            class_prompt_texts.append(_default_prompt(cls))\n\n    clip_model.to(device)\n\n    family_detector = FamilyAdaptedClipDetector(\n        name=\"global\",\n        classes=classnames,\n        clip_model=clip_model,\n        preprocess=preprocess,\n        device=device,\n        feature_dropout=config.classifier_dropout,\n        train_backbone=True,\n        text_encoder=text_encoder,\n    )\n    ctx_init = compute_init_ctx(\n        n_ctx=16,\n        text_encoder=text_encoder,\n        tokenizer=tokenizer,\n        class_prompts=class_prompt_texts,\n    )\n\n    prompt_learner = ClipPromptLearner(\n        classnames=classnames,\n        text_encoder=text_encoder,\n        ctx_vectors=ctx_init,\n        model_name=model_name,\n    ).to(device)\n\n    trainer = ClipFineTuneEngine(\n        family_detector=family_detector,\n        prompt_learner=prompt_learner\n    )\n\n    fit_args = {\n        \"epochs\": config.epochs,\n        \"batch_size\": config.batch_size,\n        \"lr\": config.lr,\n        \"log_fn\": lambda msg: print(f\"[GLOBAL] {msg}\"),\n    }\n\n    result = benchmark.run(\n        trainer=trainer,\n        segment_fn=segment_fn,\n        families=families,\n        perc_eval=None,\n        fit_predictor_args=fit_args,\n    )\n\n    test_metrics = None\n    result[\"test_samples\"] = 0\n    try:\n        test_dataset = PlantData(\n            dataset_root=config.dataset_path,\n            families=families,\n            split=\"test\",\n            segment_fn=segment_fn,\n            transform=preprocess,\n        )\n        test_loader = torch.utils.data.DataLoader(\n            test_dataset,\n            batch_size=config.batch_size,\n            shuffle=False,\n        )\n        result[\"test_samples\"] = len(test_dataset)\n        test_epoch = trainer.eval(test_loader)\n        test_metrics = {\"loss\": test_epoch.loss, \"f1\": test_epoch.f1}\n        result[\"test_metrics\"] = test_metrics\n    except FileNotFoundError as exc:\n        print(f\"[WARN] Test split unavailable: {exc}\")\n        result[\"test_metrics\"] = None\n\n    try:\n        lora_dir = Path(benchmark.run_dir) / \"lora\"\n        lora_dir.mkdir(parents=True, exist_ok=True)\n        clip_model.visual.save_pretrained(lora_dir)\n        print(f\"[LoRA] adapter saved to {lora_dir}\")\n    except Exception as e:\n        print(f\"[LoRA][WARN] failed to save adapter: {e}\")\n\n    embeddings_dir = (\n        config.embeddings_dir\n        if config.embeddings_dir is not None\n        else Path(benchmark.run_dir) / \"embeddings\"\n    )\n    embeddings_dir.mkdir(parents=True, exist_ok=True)\n    embedding_file = embeddings_dir / f\"{config.run_id}_{family_detector.name}.pt\"\n    export_family_embeddings(\n        embedding_file,\n        family_detector.name,\n        classnames,\n        prompt_learner,\n        text_encoder,\n        temperature=family_detector.temperature,\n    )\n    print(f\"[PROMPTS] embeddings exported to {embedding_file}\")\n\n    eval_metrics = result.get(\"eval_metrics\")\n    test_metrics = result.get(\"test_metrics\")\n    summary_row = {\n        \"family_id\": \"global\",\n        \"train_samples\": result[\"train_samples\"],\n        \"eval_samples\": result[\"eval_samples\"],\n        \"test_samples\": result[\"test_samples\"],\n        \"eval_loss\": eval_metrics[\"loss\"] if eval_metrics else None,\n        \"eval_f1\": eval_metrics[\"f1\"] if eval_metrics else None,\n        \"test_loss\": test_metrics[\"loss\"] if test_metrics else None,\n        \"test_f1\": test_metrics[\"f1\"] if test_metrics else None,\n        \"temperature\": result.get(\"temperature\"),\n    }\n\n    csv_path = Path(benchmark.run_dir) / \"results.csv\"\n    fieldnames = list(summary_row.keys())\n    with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n        writer = csv.DictWriter(f, fieldnames=fieldnames)\n        writer.writeheader()\n        writer.writerow(summary_row)\n    print(f\"Results saved to {csv_path}\")\n\n    _last_run_context = {\n        \"config\": config,\n        \"families\": families,\n        \"segment_fn\": segment_fn,\n        \"preprocess\": preprocess,\n        \"family_detector\": family_detector,\n        \"prompt_learner\": prompt_learner,\n        \"classnames\": classnames,\n    }\n    return result"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Execute with Default Arguments\n\nReset `sys.argv` (so argparse ignores notebook-specific flags) and launch the experiment. Edit the list if you want to pass custom CLI options."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "notebook_args = ['clip_lora_prompting_experiment.py']\nif len(sys.argv) > 1:\n    sys.argv = notebook_args\n\nresult = run_clip_lora_prompting_experiment()\nresult"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Confusion Matrix\n\nAfter running the experiment you can visualize how predictions distribute across classes. Change `split_to_plot` to `\"test\"` if you also want to inspect the test split."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef plot_confusion_matrix_for_split(split=\"val\"):\n    if not _last_run_context:\n        raise RuntimeError(\"Run the experiment cell first so the context is available.\")\n\n    ctx = _last_run_context\n    dataset = PlantData(\n        dataset_root=ctx[\"config\"].dataset_path,\n        families=ctx[\"families\"],\n        split=split,\n        segment_fn=ctx[\"segment_fn\"],\n        transform=ctx[\"preprocess\"],\n    )\n    if len(dataset) == 0:\n        raise ValueError(f\"Split '{split}' has no samples.\")\n\n    loader = torch.utils.data.DataLoader(\n        dataset,\n        batch_size=ctx[\"config\"].batch_size,\n        shuffle=False,\n    )\n\n    detector = ctx[\"family_detector\"]\n    prompt_learner = ctx[\"prompt_learner\"]\n    prompts_embeds = None\n    tokenized_prompts = None\n    if prompt_learner is not None:\n        prompt_learner.eval()\n        with torch.no_grad():\n            prompts_embeds, tokenized_prompts = prompt_learner()\n\n    num_classes = len(ctx[\"classnames\"])\n    cm = torch.zeros((num_classes, num_classes), dtype=torch.int64)\n    detector.classifier.eval()\n\n    for xb, yb in loader:\n        xb = xb.to(detector.device)\n        yb = yb.to(detector.device)\n        with torch.no_grad():\n            logits = detector.logits(\n                xb,\n                prompts_embeds=prompts_embeds,\n                tokenized_prompts=tokenized_prompts,\n            )\n        preds = logits.argmax(dim=-1)\n        idx = (yb.view(-1) * num_classes + preds.view(-1)).to(torch.long).cpu()\n        cm += torch.bincount(idx, minlength=num_classes * num_classes).view(num_classes, num_classes)\n\n    fig, ax = plt.subplots(figsize=(8, 8))\n    im = ax.imshow(cm.numpy(), interpolation=\"nearest\", cmap=\"Blues\")\n    ax.set_title(f\"Confusion matrix ({split} split)\")\n    ax.set_xlabel(\"Predicted class\")\n    ax.set_ylabel(\"True class\")\n    tick_marks = np.arange(num_classes)\n    ax.set_xticks(tick_marks)\n    ax.set_yticks(tick_marks)\n    ax.set_xticklabels(ctx[\"classnames\"], rotation=90)\n    ax.set_yticklabels(ctx[\"classnames\"])\n    fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n\n    cm_np = cm.numpy()\n    max_val = cm_np.max() if cm_np.size else 1\n    for i in range(num_classes):\n        for j in range(num_classes):\n            value = int(cm_np[i, j])\n            color = \"white\" if value > max_val * 0.5 else \"black\"\n            ax.text(j, i, value, ha=\"center\", va=\"center\", color=color, fontsize=8)\n\n    fig.tight_layout()\n    plt.show()\n    return cm\n\n\nsplit_to_plot = \"val\"  # change to \"test\" to inspect the held-out split\nconfusion_matrix = plot_confusion_matrix_for_split(split=split_to_plot)\nconfusion_matrix"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}